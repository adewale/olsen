# Olsen Performance Analysis Tools Specification

**Version**: 1.0
**Date**: 2025-10-06
**Status**: Implemented

## Overview

This document specifies the performance analysis tooling for the Olsen photo indexer. These tools consume the JSON performance data generated by the `--perfstats` flag and provide insights for optimization, regression testing, and production monitoring.

## Goals

1. **Comparative Analysis**: Compare performance between different datasets or code versions
2. **Targeted Analysis**: Analyze performance for specific file types or patterns
3. **Zero Dependencies**: Use only Python standard library for maximum portability
4. **Actionable Output**: Provide clear, human-readable insights for optimization decisions
5. **Automation Ready**: Machine-parseable output for CI/CD integration

## Architecture

### Tool Suite

The performance analysis tools are located in `perftools/` and consist of:

| Tool | Purpose | Primary Use Case |
|------|---------|------------------|
| `compare_datasets.py` | Compare two perfstats files | Regression testing, before/after optimization analysis |
| `analyze_filetype.py` | Analyze specific file types | Identify file-type-specific bottlenecks, production profiling |

### Data Flow

```
┌─────────────────────┐
│  olsen index        │
│  --perfstats        │
└──────────┬──────────┘
           │
           │ Generates
           ▼
┌─────────────────────┐
│ perfstats_TIMESTAMP │
│ .json               │
└──────────┬──────────┘
           │
           │ Consumed by
           ▼
┌─────────────────────┐     ┌─────────────────────┐
│ compare_datasets.py │ OR  │ analyze_filetype.py │
└──────────┬──────────┘     └──────────┬──────────┘
           │                           │
           │ Output                    │ Output
           ▼                           ▼
┌─────────────────────┐     ┌─────────────────────┐
│ Performance         │     │ File Type           │
│ Comparison Report   │     │ Analysis Report     │
└─────────────────────┘     └─────────────────────┘
```

## Tool Specifications

### compare_datasets.py

**Purpose**: Compare performance statistics between two indexing runs

**Usage**:
```bash
python3 perftools/compare_datasets.py <baseline.json> <current.json>
```

**Input**: Two perfstats JSON files (baseline and comparison)

**Output**: Comparative analysis report showing:
1. Dataset size comparison (photo count, total MB)
2. Throughput comparison (MB/s)
3. Pipeline stage breakdown (% of total time)
4. Absolute timing changes (milliseconds)
5. Percentage changes per stage
6. Total per-photo timing with delta

**Output Format**:
```
PERFORMANCE COMPARISON: Small vs Large Dataset
==================================================================

Dataset Size:              13 photos  vs     1189 photos
Total Data:              253.8 MB     vs  1111.0 MB
Throughput:               16.73 MB/s   vs     1.39 MB/s

PIPELINE STAGE BREAKDOWN (% of total time):
------------------------------------------------------------------
Stage              Small %    Large %         Δ
------------------------------------------------------------------
Hash                  0.74%      0.79%     +0.05%
Metadata              0.82%      1.14%     +0.32%
Image Decode         42.00%     55.00%    +13.00%
Thumbnails           51.84%     11.34%    -40.50%
Color Extract         4.43%     29.70%    +25.27%
Perceptual Hash       0.07%      0.16%     +0.09%
Inference             0.00%      0.01%     +0.01%
Database              0.07%      1.28%     +1.21%

ABSOLUTE TIMINGS (milliseconds per photo):
------------------------------------------------------------------
Stage              Small     Large         Δ    % Change
------------------------------------------------------------------
Hash                8.69ms     6.04ms    -2.65ms     -30.5%
Metadata            9.54ms     8.75ms    -0.79ms      -8.3%
Image Decode      490.08ms   421.77ms   -68.31ms     -13.9%
Thumbnails        604.92ms    87.02ms  -517.90ms     -85.6%
Color Extract      51.69ms   227.86ms  +176.17ms    +340.8%
Perceptual Hash     0.85ms     1.22ms    +0.37ms     +43.5%
Inference           0.00ms     0.09ms    +0.09ms  +18900.0%
Database            0.77ms     9.83ms    +9.06ms   +1176.6%

Total per photo:  1166.92ms   767.16ms  -399.76ms     -34.3%
```

**Implementation Details**:
- Single Python file, ~90 lines
- Uses only `json` and `sys` standard library modules
- Loads both JSON files into memory
- Calculates deltas and percentage changes
- Formats output with aligned columns

**Use Cases**:
1. **Regression Testing**: Compare before/after code changes
2. **Dataset Scaling Analysis**: Understand performance at different scales
3. **Optimization Verification**: Confirm optimization achieved expected gains

**Example Workflows**:
```bash
# Establish baseline
./bin/olsen index --db baseline.db --w 4 --perfstats testdata/dng
cp perfstats_*.json baseline_perfstats.json

# After code changes
./bin/olsen index --db new.db --w 4 --perfstats testdata/dng

# Compare
python3 perftools/compare_datasets.py baseline_perfstats.json perfstats_*.json
```

### analyze_filetype.py

**Purpose**: Analyze performance for specific file types or filename patterns

**Usage**:
```bash
# Analyze by file extension
python3 perftools/analyze_filetype.py <perfstats.json> ".DNG"

# Analyze by pattern (substring matching)
python3 perftools/analyze_filetype.py <perfstats.json> "L10" --pattern
```

**Input**:
- Perfstats JSON file
- File extension (e.g., `.DNG`, `.jpeg`) or pattern string
- Optional `--pattern` flag for substring matching instead of extension matching

**Output**: Comprehensive analysis report showing:
1. Count and total size statistics
2. Average timings per photo for each pipeline stage
3. Percentage breakdown showing primary bottlenecks
4. Distribution statistics (min, median, P95, max)
5. Variability analysis (coefficient of variation)

**Output Format**:
```
LEICA L10 DNG ANALYSIS (205 photos)
======================================================================

Count:              205 photos
Total Size:         14782.0 MB
Avg File Size:      72.1 MB
Throughput:         27.11 MB/s

AVERAGE TIMINGS PER PHOTO:
----------------------------------------------------------------------
Stage              Time (ms)  % of Total
----------------------------------------------------------------------
Hash                   8.93ms       0.33%
Metadata               6.99ms       0.26%
Image Decode        2393.15ms      88.82%
Thumbnails            53.12ms       1.97%
Color Extract        160.21ms       5.94%
Perceptual Hash        1.13ms       0.04%
Inference              0.00ms       0.00%
Database              70.79ms       2.63%
TOTAL               2694.33ms     100.00%

DISTRIBUTION STATS:
----------------------------------------------------------------------
Stage                  Min    Median       P95       Max
----------------------------------------------------------------------
Image Decode          2124ms    2363ms    2821ms    3074ms
Thumbnails              38ms      51ms      72ms      98ms
Color Extract          123ms     158ms     208ms     271ms
TOTAL                 2376ms    2670ms    3107ms    3427ms

VARIABILITY (Coefficient of Variation %):
----------------------------------------------------------------------
Database             128.3% CV
Color Extract         14.7% CV
Thumbnails            14.4% CV
Perceptual Hash       14.3% CV
Hash                  13.6% CV
Image Decode           7.2% CV
Metadata               5.8% CV
```

**Implementation Details**:
- Single Python file, ~180 lines
- Uses only `json`, `sys`, `argparse` standard library modules
- Filters photos by extension or pattern
- Calculates comprehensive statistics:
  - Arithmetic mean for averages
  - Coefficient of variation (CV) for variability: `(std_dev / mean) * 100`
  - Percentiles via sorted list indexing
- Sorts variability by CV descending to highlight inconsistent stages

**Statistical Definitions**:
- **Coefficient of Variation (CV)**: `(standard_deviation / mean) * 100%`
  - Low CV (<10%): Consistent performance
  - Medium CV (10-30%): Moderate variability
  - High CV (>30%): High variability, investigate outliers
- **P95 (95th percentile)**: 95% of photos process faster than this value
- **Median**: Middle value, more robust than mean for skewed distributions

**Use Cases**:
1. **File Type Profiling**: Understand performance characteristics of specific formats
2. **Production Analysis**: Identify which file types need optimization
3. **Outlier Detection**: Find photos with unusually slow processing times
4. **Bottleneck Identification**: Determine which stage is limiting throughput

**Example Workflows**:
```bash
# Index production photos
./bin/olsen index --db photos.db --w 4 --perfstats ~/Pictures

# Analyze Leica DNGs
python3 perftools/analyze_filetype.py perfstats_*.json "L10" --pattern

# Analyze JPEGs separately
python3 perftools/analyze_filetype.py perfstats_*.json ".jpeg"

# Analyze specific camera model
python3 perftools/analyze_filetype.py perfstats_*.json "canon_r5" --pattern
```

## Performance Characteristics

### Tool Performance

| Tool | Dataset Size | Memory Usage | Execution Time | Notes |
|------|--------------|--------------|----------------|-------|
| compare_datasets.py | 13 photos | <5 MB | <100ms | Instant |
| compare_datasets.py | 1,189 photos | ~15 MB | <500ms | Fast |
| analyze_filetype.py | 1,189 photos | ~15 MB | <500ms | Fast |

**Memory Considerations**:
- Both tools load entire JSON into memory
- For JSON files >100MB, consider pre-filtering with `jq`:
  ```bash
  jq '.detailed | map(select(.FilePath | contains("L10")))' large.json > filtered.json
  python3 perftools/analyze_filetype.py filtered.json "L10" --pattern
  ```

## Real-World Insights

### Discovery: Misleading Test Data

Performance analysis revealed a critical insight about test data representativeness:

**Synthetic Test Data (13 photos, 20MB average)**:
```
Thumbnails:      51.84% (605ms avg) - PRIMARY BOTTLENECK
Image Decode:    42.00% (490ms avg) - SECONDARY
Color Extract:    4.43% (52ms avg)  - Minor
```

**Mixed Production Data (1,189 photos, 20MB average)**:
```
Image Decode:    55.00% (422ms avg) - PRIMARY BOTTLENECK
Color Extract:   29.70% (228ms avg) - SECONDARY (340% slower!)
Thumbnails:      11.34% (87ms avg)  - Not a problem
```

**Leica DNG Production Data (205 photos, 72MB average)**:
```
Image Decode:    88.82% (2393ms avg) - OVERWHELMING BOTTLENECK
Color Extract:    5.94% (160ms avg)  - Acceptable
Thumbnails:       1.97% (53ms avg)   - Fast
Everything else:  <4%                - Negligible
```

**Key Lessons**:
1. Test data performance profile was **completely misleading**
2. Actual production bottleneck: RAW preview extraction (2.4s per 72MB Leica DNG)
3. Color extraction scaled poorly with dataset size (340% slower per photo)
4. Database operations showed lock contention at scale (1,173% slower per photo)
5. Thumbnail generation is NOT a bottleneck in production (contrary to test data)

### Optimization Priorities (Based on Real Data)

**For Leica DNGs (Primary Use Case)**:

1. **HIGH PRIORITY**: Optimize LibRaw embedded JPEG extraction
   - Current: 2.4 seconds per 72MB file (88.82% of time)
   - Target: <1.5 seconds per file
   - Potential gain: 40-60% overall speedup
   - Approaches:
     - Use faster LibRaw preview extraction method
     - Implement preview caching
     - Consider alternative RAW libraries

2. **MEDIUM PRIORITY**: Optimize color extraction
   - Current: 160ms per photo (5.94% of time)
   - High variability (CV: 14.7%)
   - Potential gain: 2-3% overall speedup
   - Approaches:
     - Reduce k-means iterations from 100 to 50
     - Use faster clustering algorithm
     - Cache results for similar images

3. **LOW PRIORITY**: Database lock contention
   - Current: 70ms per photo (2.63% of time)
   - High variability (CV: 128.3%)
   - Suggests lock contention at scale
   - Potential gain: 1-2% overall speedup
   - Approaches:
     - Batch inserts (N photos per transaction)
     - Use WAL mode for better concurrency

**Optimization Impact Estimation**:
- Optimizing image decode: ~40-60% total speedup
- Optimizing color extraction: ~2-3% total speedup
- Optimizing everything else: <2% total speedup

## Typical Workflows

### Workflow 1: After Initial Indexing

**Goal**: Identify optimization priorities

**Steps**:
```bash
# 1. Index with performance tracking
./bin/olsen index --db photos.db --w 4 --perfstats ~/Pictures

# 2. Analyze by file type
python3 perftools/analyze_filetype.py perfstats_*.json ".DNG"
python3 perftools/analyze_filetype.py perfstats_*.json ".jpeg"

# 3. Identify bottlenecks based on:
#    - Which stage has highest % of total time
#    - Which stage has highest variability (CV)
#    - File type differences
```

### Workflow 2: Performance Regression Testing

**Goal**: Ensure code changes don't degrade performance

**Steps**:
```bash
# 1. Establish baseline
./bin/olsen index --db baseline.db --w 4 --perfstats testdata/dng
cp perfstats_*.json baseline_perfstats.json

# 2. After code changes, run again
./bin/olsen index --db new.db --w 4 --perfstats testdata/dng

# 3. Compare results
python3 perftools/compare_datasets.py baseline_perfstats.json perfstats_*.json

# 4. Check for regressions:
#    - Total time per photo should not increase >10%
#    - No single stage should show >20% increase
#    - Throughput should not decrease >10%
```

### Workflow 3: Optimization Impact Analysis

**Goal**: Verify optimization achieved expected gains

**Steps**:
```bash
# 1. Before optimization
./bin/olsen index --db before.db --w 4 --perfstats ~/Pictures
mv perfstats_*.json before_perfstats.json

# 2. Implement optimization (e.g., faster thumbnail generation)

# 3. After optimization
./bin/olsen index --db after.db --w 4 --perfstats ~/Pictures
mv perfstats_*.json after_perfstats.json

# 4. Compare
python3 perftools/compare_datasets.py before_perfstats.json after_perfstats.json

# 5. Verify:
#    - Target stage shows significant reduction
#    - Total time decreased proportionally
#    - Other stages unaffected (no regressions)
```

### Workflow 4: Production Monitoring

**Goal**: Track performance over time in production

**Steps**:
```bash
# Weekly performance check
./bin/olsen index --db prod.db --w 8 --perfstats /mnt/photos

# Analyze most common file type
python3 perftools/analyze_filetype.py perfstats_*.json "L10" --pattern

# Archive results
mkdir -p perfstats_history
mv perfstats_*.json perfstats_history/perfstats_$(date +%Y%m%d).json

# Compare to last week
python3 perftools/compare_datasets.py \
    perfstats_history/perfstats_$(date -d '7 days ago' +%Y%m%d).json \
    perfstats_history/perfstats_$(date +%Y%m%d).json
```

## Quick Reference: jq Queries

For quick ad-hoc analysis without running Python scripts:

```bash
# Get summary statistics
jq '.summary' perfstats.json

# Count photos by type
jq '.detailed | map(.FilePath | split("/")[-1] | split(".")[-1]) | group_by(.) | map({type: .[0], count: length})' perfstats.json

# Find slowest photos
jq -r '.detailed | sort_by(.TotalTime) | reverse | .[0:10] | .[] | "\(.TotalTime / 1000000 | floor)ms | \(.FilePath | split("/")[-1])"' perfstats.json

# Average time by stage
jq '.summary | {
  hash: .AvgHashMs,
  metadata: .AvgMetadataMs,
  decode: .AvgImageDecodeMs,
  thumbnails: .AvgThumbnailMs,
  color: .AvgColorMs,
  phash: .AvgPerceptualHashMs,
  database: .AvgDatabaseMs
}' perfstats.json

# Check for performance regressions
jq -r '.summary.AvgTotalMs' perfstats_new.json | \
  awk -v baseline=$(jq '.summary.AvgTotalMs' perfstats_baseline.json) \
      '{if ($1 > baseline * 1.2) print "REGRESSION: " ($1/baseline-1)*100 "% slower"}'

# Filter to specific file type
jq '.detailed | map(select(.FilePath | contains("L10")))' perfstats.json > filtered.json
```

## Understanding the Output

### Key Metrics

- **Total Time**: End-to-end processing time per photo
- **% of Total**: Percentage of time spent in each pipeline stage (identifies bottlenecks)
- **Throughput**: MB/s of data processed (includes I/O, lower at scale due to contention)
- **CV (Coefficient of Variation)**: Measure of variability (higher = less consistent)
- **P95**: 95th percentile, useful for capacity planning (95% of photos faster than this)
- **Median**: Middle value, more robust than mean for skewed distributions

### Performance Targets

For production use (Leica DNG files, 70MB average):

| Metric | Target | Current (Baseline) | Status |
|--------|--------|-------------------|--------|
| Total time per photo | <2000ms | ~2700ms | ⚠️ NEEDS OPTIMIZATION |
| Throughput | >30 MB/s | ~27 MB/s | ⚠️ NEEDS OPTIMIZATION |
| Image decode % | <80% | ~89% | ⚠️ BOTTLENECK |
| Color extract % | <5% | ~6% | ✅ ACCEPTABLE |

### Interpreting Results

**High % of Total** = Primary bottleneck
- Focus optimization efforts here first
- Even small improvements have large impact
- Example: 10% improvement to 88% bottleneck = 8.8% total speedup

**High CV %** = Inconsistent performance
- May indicate algorithmic issues (e.g., k-means iteration count varies by image)
- May indicate file-specific issues (corruption, complexity)
- Investigate outliers individually

**Large P95-Median gap** = Outliers present
- Investigate slowest photos specifically
- May need special handling for edge cases
- Could indicate disk I/O issues or thermal throttling

## Testing

### Verification Checklist

After creating or modifying performance tools:

1. **Functionality**:
   - [ ] Tool runs without errors on small dataset (13 photos)
   - [ ] Tool runs without errors on large dataset (1,189 photos)
   - [ ] All statistics are calculated correctly
   - [ ] Output is human-readable and well-formatted

2. **Edge Cases**:
   - [ ] Handles empty datasets gracefully
   - [ ] Handles single-photo datasets
   - [ ] Handles missing fields in JSON
   - [ ] Handles zero values (division by zero)

3. **Performance**:
   - [ ] Executes in <1 second for typical datasets
   - [ ] Memory usage reasonable (<50MB for large datasets)

### Test Commands

```bash
# Test compare_datasets.py
python3 perftools/compare_datasets.py perfstats_20251006_131024.json perfstats_20251006_131939.json

# Test analyze_filetype.py with extension
python3 perftools/analyze_filetype.py perfstats_20251006_131024.json ".dng"

# Test analyze_filetype.py with pattern
python3 perftools/analyze_filetype.py perfstats_20251006_131939.json "L10" --pattern

# Test with no matches
python3 perftools/analyze_filetype.py perfstats_20251006_131024.json ".xyz"
# Expected: "No photos found matching pattern!"
```

## Troubleshooting

### Issue: "No photos found matching pattern"

**Causes**:
- File extension case mismatch (`.DNG` vs `.dng`)
- Pattern not present in filenames
- Perfstats file contains no detailed data

**Solutions**:
```bash
# Check file extensions in data
jq '.detailed[].FilePath' perfstats.json | head

# Try pattern matching instead
python3 perftools/analyze_filetype.py perfstats.json "dng" --pattern

# Verify perfstats has detailed data
jq '.detailed | length' perfstats.json
```

### Issue: Large memory usage

**Cause**: Python loads entire JSON into memory

**Solutions**:
```bash
# For >100MB JSON files, pre-filter with jq
jq '.detailed | map(select(.FilePath | contains("L10")))' large.json > filtered.json
python3 perftools/analyze_filetype.py filtered.json "L10" --pattern

# Or use jq for all analysis
jq -r '.detailed | map(select(.FilePath | contains("L10"))) |
       {count: length, avg_total: (map(.TotalTime) | add / length / 1000000)}' perfstats.json
```

### Issue: Inconsistent results between runs

**Causes**:
- System under load during benchmarking
- Disk cache effects (first run vs subsequent)
- CPU thermal throttling
- Background processes

**Solutions**:
```bash
# Run multiple times and average
for i in {1..3}; do
  ./bin/olsen index --db test_$i.db --w 4 --perfstats testdata/dng
done

# Compare all three runs
python3 perftools/compare_datasets.py perfstats_1.json perfstats_2.json
python3 perftools/compare_datasets.py perfstats_2.json perfstats_3.json

# Clear disk cache before benchmarking (macOS)
sudo purge

# Monitor CPU temperature
sudo powermetrics --samplers cpu_power -i 1000 -n 1
```

## Implementation Details

### Code Locations

- **Analysis tools**: `perftools/compare_datasets.py`, `perftools/analyze_filetype.py`
- **Tool documentation**: `perftools/README.md`
- **Tool specification**: `docs/perftools.spec` (this file)
- **Performance data generator**: `internal/indexer/indexer.go`, `internal/indexer/perfoutput.go`
- **Performance specification**: `docs/performance.spec`

### Design Principles

1. **Zero Dependencies**: Only Python 3 standard library
   - Easy to run in any environment
   - No `pip install` required
   - Works in CI/CD without setup

2. **Single-File Tools**: Each tool is a single Python file
   - Easy to understand and modify
   - Can be run directly with `python3`
   - Self-contained logic

3. **Human-Friendly Output**: Aligned columns, clear labels, visual separators
   - Can be read directly by developers
   - Can be pasted into bug reports or documentation
   - No need for additional formatting

4. **Machine-Parseable Input**: Consumes structured JSON
   - Easy to automate
   - Can be piped to other tools
   - Supports CI/CD integration

5. **Fail-Fast**: Clear error messages on invalid input
   - File not found: exit with error
   - Invalid JSON: exit with error
   - No matching photos: print warning and exit

## Future Enhancements

### Planned

1. **Additional Analysis Tools**:
   - `analyze_workers.py` - Compare performance across different worker counts
   - `analyze_trends.py` - Track performance over time from multiple perfstats files
   - `export_prometheus.py` - Export metrics in Prometheus format

2. **Enhanced Statistics**:
   - Histogram generation (ASCII art or matplotlib)
   - Correlation analysis (file size vs processing time)
   - Outlier detection with automatic flagging

3. **CI/CD Integration**:
   - `check_regression.py` - Exit with error code if regression detected
   - GitHub Actions workflow for automatic performance testing
   - Performance badges for README

### Under Consideration

1. **Interactive Dashboard**: Real-time visualization of performance data
2. **Optimization Recommender**: Suggest configuration changes based on analysis
3. **Export Formats**: CSV, HTML reports, PDF generation
4. **Database Integration**: Store performance history in SQLite for long-term tracking

## Contributing

When adding new analysis tools:

1. **Naming Convention**: Use `verb_noun.py` format (e.g., `analyze_filetype.py`, `compare_datasets.py`)
2. **Documentation**: Include docstring with usage examples
3. **Command-Line Interface**: Use `argparse` for options, filename as first positional argument
4. **Output Format**: Human-readable to stdout, machine-readable optional
5. **Error Handling**: Fail gracefully with clear error messages
6. **Update Documentation**: Add usage examples to `perftools/README.md`
7. **Update This Spec**: Document new tool in this specification file

## References

- Performance tracking implementation: `docs/performance.spec`
- Indexer architecture: `internal/indexer/indexer.go`
- Performance data structures: `pkg/models/types.go:139-189`
- JSON output format: `internal/indexer/perfoutput.go`
- LibRaw documentation: https://www.libraw.org/docs
- Python argparse: https://docs.python.org/3/library/argparse.html
- jq manual: https://stedolan.github.io/jq/manual/

## Changelog

### Version 1.0 (2025-10-06)
- Initial implementation
- Created `compare_datasets.py` for before/after analysis
- Created `analyze_filetype.py` for file-type-specific profiling
- Discovered Leica DNG bottleneck (88.82% in image decode, 2.4s per photo)
- Identified test data as misleading (thumbnail bottleneck was synthetic)
- Established performance targets based on real production data
- Documented typical workflows for regression testing and optimization
